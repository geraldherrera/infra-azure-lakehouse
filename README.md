# ğŸ“¦ Infrastructure Lakehouse â€“ DÃ©ploiement via Terraform

Ce dÃ©pÃ´t contient l'automatisation complÃ¨te de l'infrastructure pour un projet de type Lakehouse sur Azure. Le dÃ©ploiement est basÃ© sur Terraform, avec un script Python de post-dÃ©ploiement obligatoire pour complÃ©ter la configuration du workspace Databricks.

## âœ… FonctionnalitÃ©s

- CrÃ©ation de groupes de ressources dÃ©diÃ©s
- DÃ©ploiement d'un SQL Server + base de donnÃ©es avec exemple AdventureWorksLT
- CrÃ©ation d'un Azure Key Vault contenant les identifiants SQL
- Provisionnement d'un Azure Databricks Workspace avec son managed resource group
- GÃ©nÃ©ration d'outputs Terraform rÃ©utilisables pour automatisation
- Stockage des notebooks `.dbc` versionnÃ©s dans `/notebooks/` :
  - `1. Initialisation.dbc`
  - `2. Bronze layer â€“ Ingestion.dbc`
  - `3. Silver layer â€“ Transformation.dbc`
  - `4. Gold layer â€“ Aggregation.dbc`

## ğŸš€ DÃ©ploiement en 3 Ã©tapes

### 1. Initialisation et crÃ©ation de l'infrastructure

```bash
git clone https://github.com/geraldherrera/infra-azure-lakehouse.git
cd infra-azure-lakehouse
```

CrÃ©er un fichier `secrets.auto.tfvars` avec les variables sensibles :

```hcl
subscription_id     = "<votre-subscription-id>"
sql_admin           = "<nom-utilisateur-sql>"
sql_password        = "<mot-de-passe-sql>"
databricks_token    = "<token-databricks>"
```

> âš ï¸ Ce fichier ne doit **jamais Ãªtre versionnÃ©**. Il est ignorÃ© par `.gitignore`.

Puis lancer lâ€™init et lâ€™application du plan Terraform :

```bash
terraform init
terraform apply
```

### 2. GÃ©nÃ©ration manuelle du token Databricks

AprÃ¨s la crÃ©ation du workspace Databricks :
1. Connectez-vous Ã  lâ€™interface Databricks
2. Allez dans `User Settings > Access Tokens`
3. Cliquez sur "Generate New Token"
4. Copiez ce token dans `secrets.auto.tfvars` sous la clÃ© `databricks_token`

### 3. Lancement du post-dÃ©ploiement

Un script `post_deploy.py` est requis pour :
- Importer automatiquement les notebooks `.dbc` dans le workspace Databricks
- (Ã€ venir) crÃ©er les jobs et orchestrer le workflow complet

## ğŸ“ Structure du dÃ©pÃ´t

```
infra-azure-lakehouse/
â”œâ”€â”€ main.tf                  # DÃ©ploiement de l'infrastructure Azure
â”œâ”€â”€ variables.tf             # Variables globales
â”œâ”€â”€ outputs.tf               # Infos utiles extraites aprÃ¨s apply
â”œâ”€â”€ secrets.auto.tfvars      # âš ï¸ Fichier local, non versionnÃ©
â”œâ”€â”€ notebooks/               # Notebooks .dbc prÃªts Ã  importer
â”‚   â”œâ”€â”€ 1. Initialisation.dbc
â”‚   â”œâ”€â”€ 2. Bronze layer â€“ Ingestion.dbc
â”‚   â”œâ”€â”€ 3. Silver layer â€“ Transformation.dbc
â”‚   â””â”€â”€ 4. Gold layer â€“ Aggregation.dbc
â”œâ”€â”€ post_deploy.py           # Script Python pour automatisation Databricks
â”œâ”€â”€ README.md                # Ce fichier
```

## ğŸ’¬ Ã€ venir

- CrÃ©ation dâ€™un cluster et dâ€™une policy "Personal Compute"
- IntÃ©gration dâ€™un workflow Databricks (jobs)
- Gestion dâ€™un train/test en sandbox ML

---

ğŸ› ï¸ Ce projet a Ã©tÃ© conÃ§u pour minimiser les manipulations manuelles et garantir la reproductibilitÃ© du dÃ©ploiement sur Azure + Databricks. Il peut servir de base Ã  toute architecture de type Lakehouse en environnement acadÃ©mique ou professionnel.
