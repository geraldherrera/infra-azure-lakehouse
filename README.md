# ğŸ“¦ Infrastructure Lakehouse â€“ DÃ©ploiement via Terraform

Ce dÃ©pÃ´t contient l'automatisation complÃ¨te de l'infrastructure pour un projet de type Lakehouse sur Azure. Le dÃ©ploiement est basÃ© sur Terraform, avec un script Python de post-dÃ©ploiement permettant de finaliser la configuration du workspace Databricks et d'y importer les notebooks.

## âœ… FonctionnalitÃ©s

- CrÃ©ation de groupes de ressources dÃ©diÃ©s pour SQL et Databricks
- DÃ©ploiement d'un SQL Server avec base de donnÃ©es AdventureWorksLT (format Ã©tudiant)
- CrÃ©ation d'un Azure Key Vault contenant les identifiants SQL
- Provisionnement d'un Azure Databricks Workspace avec managed resource group
- GÃ©nÃ©ration d'outputs Terraform rÃ©utilisables dans les scripts
- Stockage des notebooks `.ipynb` versionnÃ©s dans le dossier `/notebooks/` :
  - `1.0_initialisation.ipynb`
  - `2.0_bronze_layer_ingestion.ipynb`
  - `2.5_bronze_layer_test.ipynb`
  - `3.0_silver_layer_transformation.ipynb`
  - `3.5_silver_layer_test.ipynb`
  - `4.0_gold_layer_aggregation.ipynb`

## ğŸš€ DÃ©ploiement en 3 Ã©tapes

### 1. Initialisation et crÃ©ation de l'infrastructure

```bash
git clone https://github.com/geraldherrera/infra-azure-lakehouse.git
cd infra-azure-lakehouse
```

CrÃ©er un fichier `secrets.auto.tfvars` (non versionnÃ©) avec les variables sensibles :

```hcl
subscription_id     = "<votre-subscription-id>"
sql_admin           = "<nom-utilisateur-sql>"
sql_password        = "<mot-de-passe-sql>"
aad_admin_login     = "<votre-email@domain.com>"
aad_admin_object_id = "<object-id-de-votre-utilisateur>"
```

Et un fichier `terraform.tfvars` pour les noms de ressources :

```hcl
location                    = "westeurope"
location_sql                = "switzerlandnorth"
rg_datasource_name          = "rg-datasource-dev-ghe"
rg_dataplatform_name        = "rg-dataplatform-dev-ghe"
sql_server_name             = "sql-datasource-dev-ghe"
sql_database_name           = "sqldb-adventureworks-dev-ghe"
key_vault_name              = "kv-jdbc-secrets-dev-ghe"
databricks_workspace_name   = "dbw-dataplatform-dev-ghe"
databricks_managed_rg_name  = "mg-dataplatform-dev-ghe"
```

Puis lancer l'initialisation et l'application du plan :

```bash
terraform init
terraform apply
```

### 2. CrÃ©ation manuelle du scope Azure Key Vault dans Databricks

Une seule Ã©tape manuelle est nÃ©cessaire pour lier le Key Vault Ã  Databricks :

```bash
# RÃ©cupÃ©rer un token AAD
export DATABRICKS_AAD_TOKEN=$(az account get-access-token --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d --query accessToken -o tsv)

# Configurer la CLI Databricks avec AAD
databricks configure --aad-token

# CrÃ©er le scope kv-jdbc
# Remplacer les valeurs avec celles de vos outputs Terraform

databricks secrets create-scope \
  --scope kv-jdbc \
  --scope-backend-type AZURE_KEYVAULT \
  --resource-id "/subscriptions/<subscription-id>/resourceGroups/<nom-rg>/providers/Microsoft.KeyVault/vaults/<nom-vault>" \
  --dns-name "https://<nom-vault>.vault.azure.net/"
```

> âš ï¸ Cette suite de commande est Ã  exÃ©cuter **une seule fois**. Elle lie Databricks Ã  votre Key Vault de faÃ§on permanente.

### 3. Lancement du post-dÃ©ploiement

Le script `post_deploy.py` effectue les actions suivantes :
- CrÃ©e la cluster policy "Personal Policy - GHE"
- CrÃ©e un cluster "Personal Compute - Gerald Herrera"
- Importe les notebooks `.ipynb` dans Databricks
- ExÃ©cute automatiquement le notebook `1. Initialisation`

```bash
python post_deploy.py
```

> Il vous sera demandÃ© de saisir votre token d'accÃ¨s personnel Databricks lors de l'exÃ©cution du script.

## ğŸ“ Structure du dÃ©pÃ´t

```
infra-azure-lakehouse/
â”œâ”€â”€ main.tf                     # DÃ©ploiement de l'infrastructure Azure
â”œâ”€â”€ variables.tf                # Variables globales
â”œâ”€â”€ outputs.tf                  # Infos extraites automatiquement
â”œâ”€â”€ secrets.auto.tfvars         # âš ï¸ Fichier local, non versionnÃ©
â”œâ”€â”€ terraform.tfvars            # Valeurs des noms de ressources
â”œâ”€â”€ notebooks/                  # Notebooks Databricks (.ipynb)
â”‚   â”œâ”€â”€ 1.0_initialisation.ipynb
â”‚   â”œâ”€â”€ 2.0_bronze_layer_ingestion.ipynb
â”‚   â”œâ”€â”€ 2.5_bronze_layer_test.ipynb
â”‚   â”œâ”€â”€ 3.0_silver_layer_transformation.ipynb
â”‚   â”œâ”€â”€ 3.5_silver_layer_test.ipynb
â”‚   â””â”€â”€ 4.0_gold_layer_aggregation.ipynb
â”œâ”€â”€ post_deploy.py              # Script Python de post-dÃ©ploiement
â”œâ”€â”€ alteration_donee_source.sql # Code SQL d'altÃ©ration de la base de donnÃ©es source pour tester la couche silver
â”œâ”€â”€ rollback_donee_source.sql   # Code SQL remise par dÃ©faut de la base de donnÃ©es source
â”œâ”€â”€ gold_ddl_diagram.md         # Diagram DDL de la couche gold en mermaid
â”œâ”€â”€ gold_ddl_diagram.svg        # Export svg du Diagram DDL de la couche gold
â”œâ”€â”€ README.md                   # Ce fichier
```

## ğŸ’¬ Notes

- Le dÃ©ploiement est Ã  la fois modulaire et rÃ©utilisable pour d'autres projets similaires
- L'Ã©tape manuelle de crÃ©ation du secret scope est volontaire, car elle nÃ©cessite une authentification AAD non automatisable
- Le script Python est conÃ§u pour fonctionner avec **zÃ©ro modification** si les fichiers `.tfvars` sont correctement remplis

---

ğŸ› ï¸ Ce projet a Ã©tÃ© conÃ§u pour minimiser les manipulations manuelles et garantir la reproductibilitÃ© du dÃ©ploiement sur Azure + Databricks. Il peut servir de base Ã  toute architecture de type Lakehouse en environnement acadÃ©mique ou professionnel.

---

README gÃ©nÃ©rÃ© Ã  l'aide de ChatGPT