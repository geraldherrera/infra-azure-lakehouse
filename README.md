# ğŸ“¦ Infrastructure Lakehouse â€“ DÃ©ploiement via Terraform

Ce dÃ©pÃ´t contient l'automatisation complÃ¨te de l'infrastructure pour un projet de type Lakehouse sur Azure. Le dÃ©ploiement est basÃ© sur Terraform, avec un script Python de post-dÃ©ploiement permettant de finaliser la configuration du workspace Databricks et d'y importer les notebooks.

## âœ… FonctionnalitÃ©s

- CrÃ©ation de groupes de ressources dÃ©diÃ©s pour SQL et Databricks
- DÃ©ploiement d'un SQL Server avec base de donnÃ©es AdventureWorksLT (format Ã©tudiant)
- CrÃ©ation d'un Azure Key Vault contenant les identifiants SQL
- Provisionnement d'un Azure Databricks Workspace avec managed resource group
- GÃ©nÃ©ration d'outputs Terraform rÃ©utilisables dans les scripts
- Stockage des notebooks `.ipynb` versionnÃ©s dans le dossier `/notebooks/` :
  - `1.0_initialisation.ipynb`
  - `2.0_bronze_layer_ingestion.ipynb`
  - `2.5_bronze_layer_test.ipynb`
  - `3.0_silver_layer_transformation.ipynb`
  - `3.5_silver_layer_test.ipynb`
  - `4.0_gold_layer_aggregation.ipynb`

## ğŸš€ DÃ©ploiement en 3 Ã©tapes

### 1. Initialisation et crÃ©ation de l'infrastructure

```bash
git clone https://github.com/geraldherrera/infra-azure-lakehouse.git
cd infra-azure-lakehouse
```

CrÃ©er un fichier `secrets.auto.tfvars` (non versionnÃ©) avec les variables sensibles :

```hcl
subscription_id     = "<votre-subscription-id>"
sql_admin           = "<nom-utilisateur-sql>"
sql_password        = "<mot-de-passe-sql>"
aad_admin_login     = "<votre-email@domain.com>"
aad_admin_object_id = "<object-id-de-votre-utilisateur>"
```

Et un fichier `terraform.tfvars` pour les noms de ressources :

```hcl
location                    = "westeurope"
location_sql                = "switzerlandnorth"
rg_datasource_name          = "rg-datasource-dev-ghe"
rg_dataplatform_name        = "rg-dataplatform-dev-ghe"
sql_server_name             = "sql-datasource-dev-ghe"
sql_database_name           = "sqldb-adventureworks-dev-ghe"
key_vault_name              = "kv-jdbc-secrets-dev-ghe"
databricks_workspace_name   = "dbw-dataplatform-dev-ghe"
databricks_managed_rg_name  = "mg-dataplatform-dev-ghe"
```

Puis lancer l'initialisation et l'application du plan :

```bash
terraform init
terraform apply
```

### 2. CrÃ©ation manuelle du scope Azure Key Vault dans Databricks

Une seule Ã©tape manuelle est nÃ©cessaire pour lier le Key Vault Ã  Databricks :

```bash
# RÃ©cupÃ©rer un token AAD
export DATABRICKS_AAD_TOKEN=$(az account get-access-token --resource 2ff814a6-3304-4ab8-85cb-cd0e6f879c1d --query accessToken -o tsv)

# Configurer la CLI Databricks avec AAD
databricks configure --aad-token

# CrÃ©er le scope kv-jdbc
# Remplacer les valeurs avec celles de vos outputs Terraform

databricks secrets create-scope \
  --scope kv-jdbc \
  --scope-backend-type AZURE_KEYVAULT \
  --resource-id "/subscriptions/<subscription-id>/resourceGroups/<nom-rg>/providers/Microsoft.KeyVault/vaults/<nom-vault>" \
  --dns-name "https://<nom-vault>.vault.azure.net/"
```

> Cette suite de commande est Ã  exÃ©cuter **une seule fois**. Elle lie Databricks Ã  votre Key Vault de faÃ§on permanente.

### 3. Lancement du post-dÃ©ploiement

Le script `post_deploy.py` effectue les actions suivantes :
- CrÃ©e la cluster policy "Personal Policy - GHE"
- CrÃ©e un cluster "Personal Compute - Gerald Herrera"
- Importe les notebooks `.ipynb` dans Databricks
- ExÃ©cute automatiquement le notebook `1.0_initialisation`

```bash
python post_deploy.py
```

> âš ï¸ Il vous sera demandÃ© de saisir votre token d'accÃ¨s personnel Databricks lors de l'exÃ©cution du script.

## ğŸ¯ Orchestration ETL dans Databricks (Workflow)

Une fois lâ€™infrastructure crÃ©Ã©e et les notebooks importÃ©s, un workflow Databricks est automatiquement gÃ©nÃ©rÃ© via le script `post_deploy.py`.

Ce workflow s'appelle **`lakehouse_etl_pipeline-ghe`** et sâ€™exÃ©cute automatiquement tous les jours Ã  **2h00 du matin**. Il suit une architecture Ã  trois couches, avec les tÃ¢ches suivantes :

```
| TÃ¢che              | Chemin notebook                                                              | DÃ©pendance            |
|--------------------|------------------------------------------------------------------------------|-----------------------|
| `task_bronze_ghe`  | `/Workspace/Users/gerald.herrera@he-arc.ch/2.0_bronze_layer_ingestion`       | Aucune                |
| `task_silver_ghe`  | `/Workspace/Users/gerald.herrera@he-arc.ch/3.0_silver_layer_transformation`  | `task_bronze_ghe`     |
| `task_gold_ghe`    | `/Workspace/Users/gerald.herrera@he-arc.ch/4.0_gold_layer_aggregation`       | `task_silver_ghe`     |
```

Chaque tÃ¢che est exÃ©cutÃ©e sur le cluster personnel nommÃ© **`Personal Compute - Gerald Herrera`**, prÃ©configurÃ© automatiquement par le script.

> Ce workflow assure lâ€™enchaÃ®nement cohÃ©rent des Ã©tapes du pipeline de donnÃ©es de bronze Ã  gold.

---


## âš–ï¸ AccÃ¨s Power BI via entrepÃ´t Serverless
Le script post_deploy.py crÃ©e Ã©galement un SQL Warehouse Serverless nommÃ© "Serverless SQL". Celui-ci est configurÃ© automatiquement avec les paramÃ¨tres suivants :

```
Type : Serverless

Taille : 2X-Small (XXS)

ArrÃªt automatique : 10 minutes

Min/Max clusters : 1

Canal : Current
```

Ce warehouse peut Ãªtre utilisÃ© immÃ©diatement depuis Power BI en tant que source de donnÃ©es directe, sans manipulation supplÃ©mentaire.

Pour se connecter, utilisez le connecteur Databricks dans Power BI et sÃ©lectionnez l'entrepÃ´t "Serverless SQL" comme cible.

ğŸ–ï¸ Tableau de bord Power BI

Le fichier business_sales_dashboard.pbix est un tableau de bord interactif Power BI fournissant une vue dâ€™ensemble complÃ¨te des ventes, des produits, des clients et des opÃ©rations commerciales.

Il sâ€™appuie sur un modÃ¨le en Ã©toile construit Ã  partir des donnÃ©es traitÃ©es en couche Gold sur Databricks, et couvre les thÃ©matiques suivantes :
```
Vue dâ€™ensemble commerciale : KPIs clÃ©s, tendances mensuelles, ventes par rÃ©gion.

Produits et catalogue : Analyse des meilleures ventes, produits non vendus, performance par catÃ©gorie.

Analyse client : RÃ©partition gÃ©ographique, commandes par client, clients les plus rentables.
```

Ce fichier est prÃ©vu pour Ãªtre connectÃ© directement au SQL Warehouse "Serverless SQL" dÃ©ployÃ© automatiquement. Pour cela il suffit d'ajouter la source de donnÃ©es.

## ğŸ“ Structure du dÃ©pÃ´t

```
infra-azure-lakehouse/
â”œâ”€â”€ main.tf                     # DÃ©ploiement de l'infrastructure Azure
â”œâ”€â”€ variables.tf                # Variables globales
â”œâ”€â”€ outputs.tf                  # Infos extraites automatiquement
â”œâ”€â”€ secrets.auto.tfvars         # âš ï¸ Fichier local, non versionnÃ©
â”œâ”€â”€ terraform.tfvars            # Valeurs des noms de ressources
â”œâ”€â”€ notebooks/                  # Notebooks Databricks (.ipynb)
â”‚   â”œâ”€â”€ 1.0_initialisation.ipynb
â”‚   â”œâ”€â”€ 2.0_bronze_layer_ingestion.ipynb
â”‚   â”œâ”€â”€ 2.5_bronze_layer_test.ipynb
â”‚   â”œâ”€â”€ 3.0_silver_layer_transformation.ipynb
â”‚   â”œâ”€â”€ 3.5_silver_layer_test.ipynb
â”‚   â””â”€â”€ 4.0_gold_layer_aggregation.ipynb
â”œâ”€â”€ post_deploy.py                # Script Python de post-dÃ©ploiement
â”œâ”€â”€ alteration_donee_source.sql   # Code SQL d'altÃ©ration de la base de donnÃ©es source pour tester la couche silver
â”œâ”€â”€ rollback_donee_source.sql     # Code SQL remise par dÃ©faut de la base de donnÃ©es source
â”œâ”€â”€ gold_ddl_diagram.md           # Diagram DDL de la couche gold en mermaid
â”œâ”€â”€ gold_ddl_diagram.svg          # Export svg du Diagram DDL de la couche gold
â”œâ”€â”€ business_sales_dashboard.pbix # Dashboard ventes, produits et clients (couche Gold).
â”œâ”€â”€ README.md                     # Ce fichier
```

## ğŸ’¬ Notes

- Le dÃ©ploiement est Ã  la fois modulaire et rÃ©utilisable pour d'autres projets similaires
- L'Ã©tape manuelle de crÃ©ation du secret scope est volontaire, car elle nÃ©cessite une authentification AAD non automatisable
- Le script Python est conÃ§u pour fonctionner avec **zÃ©ro modification** si les fichiers `.tfvars` sont correctement remplis

---

ğŸ› ï¸ Ce projet a Ã©tÃ© conÃ§u pour minimiser les manipulations manuelles et garantir la reproductibilitÃ© du dÃ©ploiement sur Azure + Databricks. Il peut servir de base Ã  toute architecture de type Lakehouse en environnement acadÃ©mique ou professionnel.

---

README gÃ©nÃ©rÃ© Ã  l'aide de ChatGPT