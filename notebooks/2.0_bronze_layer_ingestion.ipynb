{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0efa8e0b-a511-43d5-9f58-02cbf3b173b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paramétrage de la connexion JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1ad9c36-90cb-458b-ba31-0fc4b39b722d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres de connexion\n",
    "jdbc_hostname = \"sql-datasource-dev-ghe.database.windows.net\"\n",
    "jdbc_port = 1433\n",
    "jdbc_database = \"sqldb-adventureworks-dev-ghe\"\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};database={jdbc_database}\"\n",
    "\n",
    "# Authentification SQL\n",
    "username = dbutils.secrets.get(scope=\"kv-jdbc\", key=\"sql-username\")\n",
    "password = dbutils.secrets.get(scope=\"kv-jdbc\", key=\"sql-password\")\n",
    "\n",
    "# Options JDBC communes\n",
    "connection_properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1826f58b-c9d3-4414-9ecb-cab25aee554c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Détection dynamique du catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34e8dec5-051a-4168-8ff0-9802379dc4b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "unity_catalogs = [c for c in catalogs if c != \"hive_metastore\"]\n",
    "\n",
    "if len(unity_catalogs) == 1:\n",
    "    default_catalog = unity_catalogs[0]\n",
    "else:\n",
    "    # Choisir celui qui commence par 'dbw_' si plusieurs\n",
    "    default_catalog = next((c for c in unity_catalogs if c.startswith(\"dbw_\")), \"hive_metastore\")\n",
    "\n",
    "# Création d'un widget pour sélectionner dynamiquement le catalog\n",
    "dbutils.widgets.text(\"my_catalog\", default_catalog, \"Catalog détecté\")\n",
    "catalog = dbutils.widgets.get(\"my_catalog\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29581c5e-c306-4919-8b50-dd2040883547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Définition des schémas cibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f950e6-c55f-4df6-8bca-4f099dbea638",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schémas cibles\n",
    "bronze_schema = \"bronze\"\n",
    "logs_schema = \"logs\"\n",
    "log_table = \"bronze_processing_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275bb52d-5dca-41fc-aa6c-214423001836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Récupération dynamique des tables à ingérer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a84286-7026-4780-897e-fa942a71f090",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Récupère la liste des objets disponibles dans le schéma source\n",
    "tables_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"INFORMATION_SCHEMA.TABLES\",\n",
    "    properties=connection_properties\n",
    ")\n",
    "\n",
    "# On garde uniquement les tables physiques du schéma 'SalesLT', hors exclusions\n",
    "tables_to_ingest = (\n",
    "    tables_df\n",
    "    .filter(\"TABLE_SCHEMA = 'SalesLT'\")\n",
    "    .filter(\"TABLE_TYPE = 'BASE TABLE'\")\n",
    "    .filter(~tables_df[\"TABLE_NAME\"].isin([\"ErrorLog\", \"BuildVersion\"]))\n",
    "    .select(\"TABLE_NAME\")\n",
    "    .rdd.flatMap(lambda x: x)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b47cba-1012-4d19-8809-a1b4a9c163d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction pour journaliser les ingestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41d61d5d-28e3-40b0-bac7-181b656fff41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "\n",
    "def log_ingestion_result(table_name, status, message):\n",
    "    full_log_table = f\"{catalog}.{logs_schema}.{log_table}\"\n",
    "    log_row = Row(\n",
    "        table_name=table_name,\n",
    "        timestamp=datetime.now(),\n",
    "        status=status,\n",
    "        message=message[:5000]\n",
    "    )\n",
    "    spark.createDataFrame([log_row]) \\\n",
    "        .write.mode(\"append\") \\\n",
    "        .format(\"delta\") \\\n",
    "        .saveAsTable(full_log_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12bfc169-7c74-4b00-ba87-1885ced020b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction d'ingestion vers la couche Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437f5dbf-8394-4b74-aeec-fecfd14e7e0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, col, current_timestamp, regexp_replace\n",
    "import re\n",
    "\n",
    "# Convertit un nom CamelCase en snake_case. Exemple : ProductModelDescription -> product_model_description\n",
    "def to_snake_case(name: str) -> str:\n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "def ingest_table(table_name, source_schema=\"SalesLT\"):\n",
    "    try:\n",
    "\n",
    "        # Lecture JDBC\n",
    "        df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=f\"{source_schema}.{table_name}\",\n",
    "            properties=connection_properties\n",
    "        )\n",
    "\n",
    "        # Renommage des colonnes en snake_case\n",
    "        for field in df.schema.fields:\n",
    "            new_name = to_snake_case(field.name)\n",
    "            if field.name != new_name:\n",
    "                df = df.withColumnRenamed(field.name, new_name)\n",
    "\n",
    "        # Nettoyage des colonnes de type string\n",
    "        for field in df.schema.fields:\n",
    "            if field.dataType.simpleString() == 'string':\n",
    "                clean_col = trim(col(field.name))\n",
    "                clean_col = regexp_replace(clean_col, \"[\\\\u00A0\\\\r\\\\n]\", \"\")\n",
    "                df = df.withColumn(field.name, clean_col)\n",
    "\n",
    "        # Ajout de la colonne ingestion_timestamp\n",
    "        df = df.withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "\n",
    "        # Conversion du nom de table en snake_case\n",
    "        schema_snake = source_schema.lower()\n",
    "        table_snake = to_snake_case(table_name)\n",
    "        bronze_table_name = f\"bronze_{schema_snake}_{table_snake}\"\n",
    "        full_table_name = f\"{catalog}.{bronze_schema}.{bronze_table_name}\"\n",
    "\n",
    "        # Écriture dans Delta Lake\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .saveAsTable(full_table_name)\n",
    "\n",
    "        log_ingestion_result(bronze_table_name, \"OK\", \"Ingestion réussie\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_ingestion_result(table_name, \"KO\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afede4a-6951-48f4-951c-f9ebd87f0882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Lance la fonction d'ingestion et boucle sur chaque table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f200c7fd-76fd-4c62-ac52-5bc85bc395c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for table in tables_to_ingest:\n",
    "    ingest_table(table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6060894154855129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.0 Bronze layer - Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
