{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b6357a0-bda0-454f-8628-cd8bcb090d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Paramètrage de la connexion JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f32464-3eb6-4fa0-a431-e1fd6ee946ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Paramètres de connexion\n",
    "jdbc_hostname = \"sql-datasource-dev-ghe.database.windows.net\"\n",
    "jdbc_port = 1433\n",
    "jdbc_database = \"sqldb-adventureworks-dev-ghe\"\n",
    "jdbc_url = f\"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};database={jdbc_database}\"\n",
    "\n",
    "# Authentification SQL\n",
    "username = dbutils.secrets.get(scope=\"kv-jdbc\", key=\"sql-username\")\n",
    "password = dbutils.secrets.get(scope=\"kv-jdbc\", key=\"sql-password\")\n",
    "\n",
    "# Options JDBC communes\n",
    "connection_properties = {\n",
    "    \"user\": username,\n",
    "    \"password\": password,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baf834c-e022-4254-b8ed-b40beb598792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Détection automatique du catalog Unity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59388893-9f16-456e-afa3-0930fc76492b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "unity_catalogs = [c for c in catalogs if c != \"hive_metastore\"]\n",
    "\n",
    "if len(unity_catalogs) == 1:\n",
    "    default_catalog = unity_catalogs[0]\n",
    "else:\n",
    "    default_catalog = next((c for c in unity_catalogs if c.startswith(\"dbw_\")), \"hive_metastore\")\n",
    "\n",
    "dbutils.widgets.text(\"my_catalog\", default_catalog, \"Catalog détecté\")\n",
    "catalog = dbutils.widgets.get(\"my_catalog\")\n",
    "\n",
    "bronze_schema = \"bronze\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689a8f6e-eedb-472a-907b-61938a8a7de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Récupère toutes les colonnes d'une table dans la base source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "522ee63c-4ae6-4b03-939f-8fa3780011eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "def get_columns_for_table(table_name: str, schema: str = \"SalesLT\") -> list:\n",
    "    cols_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"INFORMATION_SCHEMA.COLUMNS\",\n",
    "        properties=connection_properties\n",
    "    ).filter(\n",
    "        (col(\"TABLE_SCHEMA\") == schema) &\n",
    "        (col(\"TABLE_NAME\") == table_name)\n",
    "    ).orderBy(\"ORDINAL_POSITION\")\n",
    "\n",
    "    return [row[\"COLUMN_NAME\"] for row in cols_df.collect()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fa33ed-1a12-4fec-ac45-9bef48903eac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction qui converti les nom en CamelCase en snake_case pour les noms de table en bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1181071c-d10e-4c3d-a412-a4521f29e65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def to_snake_case(name: str) -> str:\n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fb87f5-16a5-4ddd-b0db-612312900b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Récupère les colonnes constituant la clé primaire d'une table SQL Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f9f361-3266-4f04-a80a-d74f9b17dd1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_primary_keys(table_name: str, schema: str = \"SalesLT\") -> list:\n",
    "    key_usage_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"INFORMATION_SCHEMA.KEY_COLUMN_USAGE\",\n",
    "        properties=connection_properties\n",
    "    ).filter(\n",
    "        (col(\"TABLE_SCHEMA\") == schema) &\n",
    "        (col(\"TABLE_NAME\") == table_name)\n",
    "    )\n",
    "\n",
    "    constraints_df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=\"INFORMATION_SCHEMA.TABLE_CONSTRAINTS\",\n",
    "        properties=connection_properties\n",
    "    ).filter(\n",
    "        (col(\"TABLE_SCHEMA\") == schema) &\n",
    "        (col(\"TABLE_NAME\") == table_name) &\n",
    "        (col(\"CONSTRAINT_TYPE\") == \"PRIMARY KEY\")\n",
    "    )\n",
    "\n",
    "    primary_keys_df = key_usage_df.join(\n",
    "        constraints_df,\n",
    "        on=\"CONSTRAINT_NAME\",\n",
    "        how=\"inner\"\n",
    "    ).orderBy(\"ORDINAL_POSITION\")\n",
    "\n",
    "    return [row[\"COLUMN_NAME\"] for row in primary_keys_df.collect()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc448033-168e-41d3-92b5-589989b195f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction de récupération du nom des tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f64866-db50-4aed-8457-b43fed98f194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_tables_df = spark.read.jdbc(\n",
    "    url=jdbc_url,\n",
    "    table=\"INFORMATION_SCHEMA.TABLES\",\n",
    "    properties=connection_properties\n",
    ").filter(\"TABLE_SCHEMA = 'SalesLT' AND TABLE_TYPE = 'BASE TABLE'\")\n",
    "\n",
    "source_table_names = [row[\"TABLE_NAME\"] for row in source_tables_df.collect()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95106d4b-446a-4219-936f-eae6bdde61b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Détection des tables présentes dans la couche Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06361746-9172-4a25-90c8-44d7d70b69e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_tables_df = spark.sql(f\"SHOW TABLES IN {catalog}.{bronze_schema}\")\n",
    "bronze_table_names = [row[\"tableName\"] for row in bronze_tables_df.collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a20e7f4-8316-4aba-a3a8-d640b8774793",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Construction d'une table pour faire les tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c90190f0-5eac-49af-b255-1956cc715673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables_to_test = []\n",
    "\n",
    "for table_name in source_table_names:\n",
    "    table_snake = to_snake_case(table_name)\n",
    "    bronze_table_name = f\"bronze_saleslt_{table_snake}\"\n",
    "\n",
    "    if bronze_table_name in bronze_table_names:\n",
    "        primary_keys = get_primary_keys(table_name)\n",
    "        if not primary_keys:\n",
    "            print(f\"Aucune clé primaire détectée pour {table_name}, table ignorée.\")\n",
    "            continue\n",
    "\n",
    "        columns = get_columns_for_table(table_name)\n",
    "\n",
    "        tables_to_test.append({\n",
    "            \"source\": f\"SalesLT.{table_name}\",\n",
    "            \"bronze\": bronze_table_name,\n",
    "            \"primary_keys_source\": primary_keys,  # ex: CustomerID\n",
    "            \"primary_keys_bronze\": [to_snake_case(pk) for pk in primary_keys],  # ex: customer_id\n",
    "            \"columns_source\": columns,  # ex: Title\n",
    "            \"columns_bronze\": [to_snake_case(c) for c in columns]  # ex: title\n",
    "        })\n",
    "    else:\n",
    "        print(f\"Table non trouvée dans la couche bronze : {bronze_table_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b083844f-977a-4e64-8c74-66eb9ee49bf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction de test pour comparer la source et Bronze sur le count et les valeurs de l'échantillon aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2749e8-647c-4859-8a8a-867b352ec968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql.functions import col, trim, regexp_replace\n",
    "from functools import reduce\n",
    "\n",
    "def test_table_sample(source_table, bronze_table, primary_keys_source, primary_keys_bronze, columns_source, columns_bronze):\n",
    "    try:\n",
    "        print(f\"\\nTest de la table : {source_table} ➜ {bronze_table}\")\n",
    "\n",
    "        # COUNT total dans la source\n",
    "        count_source_total = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=source_table,\n",
    "            properties=connection_properties\n",
    "        ).count()\n",
    "\n",
    "        # COUNT total dans la Bronze\n",
    "        count_bronze_total = spark.read.table(f\"{catalog}.{bronze_schema}.{bronze_table}\").count()\n",
    "\n",
    "        # Lecture des clés depuis la source\n",
    "        source_ids_df = spark.read.jdbc(\n",
    "            url=jdbc_url,\n",
    "            table=source_table,\n",
    "            properties=connection_properties\n",
    "        ).select(*primary_keys_source).distinct()\n",
    "\n",
    "        all_rows = source_ids_df.collect()\n",
    "        if not all_rows:\n",
    "            print(f\"Table : {source_table}\\n- Aucune donnée disponible dans la source.\\nStatut : Test ignoré\\n\")\n",
    "            return\n",
    "\n",
    "        sample_rows = random.sample(all_rows, min(25, len(all_rows)))\n",
    "\n",
    "        # Construction du WHERE dynamique (à partir des noms CamelCase)\n",
    "        def format_condition(row):\n",
    "            return \"(\" + \" AND \".join([f\"{k} = {repr(row[k])}\" for k in primary_keys_source]) + \")\"\n",
    "\n",
    "        where_clause = \" OR \".join([format_condition(r) for r in sample_rows])\n",
    "        query = f\"(SELECT * FROM {source_table} WHERE {where_clause}) AS src_sample\"\n",
    "\n",
    "        # Chargement source et nettoyage\n",
    "        source_sample = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "\n",
    "        for field in source_sample.schema.fields:\n",
    "            if field.dataType.simpleString() == \"string\":\n",
    "                source_sample = source_sample.withColumn(\n",
    "                    field.name,\n",
    "                    regexp_replace(trim(col(field.name)), \"[\\\\u00A0\\\\r\\\\n]\", \"\")\n",
    "                )\n",
    "\n",
    "        # Lecture bronze\n",
    "        bronze_df = spark.read.table(f\"{catalog}.{bronze_schema}.{bronze_table}\")\n",
    "        bronze_sample = bronze_df\n",
    "\n",
    "        # Filtrage Bronze sur les valeurs de clés\n",
    "        for i, bronze_key in enumerate(primary_keys_bronze):\n",
    "            sample_values = [r[primary_keys_source[i]] for r in sample_rows]\n",
    "            bronze_sample = bronze_sample.filter(col(bronze_key).isin(sample_values))\n",
    "\n",
    "        # Jointure sur les clés\n",
    "        join_expr = reduce(lambda a, b: a & b, [\n",
    "            col(f\"src.{primary_keys_source[i]}\") == col(f\"brz.{primary_keys_bronze[i]}\")\n",
    "            for i in range(len(primary_keys_source))\n",
    "        ])\n",
    "\n",
    "        joined_df = source_sample.alias(\"src\").join(\n",
    "            bronze_sample.alias(\"brz\"),\n",
    "            on=join_expr,\n",
    "            how=\"inner\"\n",
    "        )\n",
    "\n",
    "        # Comparaison des colonnes\n",
    "        mismatches = []\n",
    "        for i, source_col in enumerate(columns_source):\n",
    "            bronze_col = columns_bronze[i]\n",
    "            if bronze_col in bronze_sample.columns:\n",
    "                diff_df = joined_df.filter(col(f\"src.{source_col}\") != col(f\"brz.{bronze_col}\"))\n",
    "                count_diff = diff_df.count()\n",
    "                if count_diff > 0:\n",
    "                    mismatches.append((source_col, count_diff))\n",
    "                    print(f\"Divergence détectée sur la colonne : {source_col} ({count_diff} ligne(s))\")\n",
    "\n",
    "                    diff_df.select(\n",
    "                        *[col(f\"src.{k}\").alias(f\"{k}_source\") for k in primary_keys_source],\n",
    "                        col(f\"src.{source_col}\").alias(f\"{source_col}_source\"),\n",
    "                        col(f\"brz.{bronze_col}\").alias(f\"{bronze_col}_bronze\")\n",
    "                    ).show(5, truncate=False)\n",
    "\n",
    "        # Résumé global\n",
    "        print(f\"\\nRésumé : {source_table}\")\n",
    "        print(f\"- Total lignes source : {count_source_total}\")\n",
    "        print(f\"- Total lignes Bronze : {count_bronze_total}\")\n",
    "        if len(primary_keys_source) == 1:\n",
    "            print(f\"- Clé primaire : {primary_keys_source[0]}\")\n",
    "        else:\n",
    "            print(f\"- Clé primaire : composite ({', '.join(primary_keys_source)})\")\n",
    "        print(f\"- Colonnes testées : {len(columns_source)}\")\n",
    "        print(f\"- Colonnes divergentes : {len(mismatches)}\")\n",
    "        if not mismatches and count_source_total == count_bronze_total:\n",
    "            print(\"Statut : Aucune divergence\\n\")\n",
    "        else:\n",
    "            print(\"Statut : Divergence détectée\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Table : {source_table}\\n- Erreur lors du test : {str(e)}\\nStatut : Divergence détectée\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e85e44-c54f-449a-ac02-b1162bf6d8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Exécution du test pour toutes les tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d450419e-4211-445b-8579-938d31719b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for t in tables_to_test:\n",
    "    try:\n",
    "        test_table_sample(\n",
    "            source_table=t[\"source\"],\n",
    "            bronze_table=t[\"bronze\"],\n",
    "            primary_keys_source=t[\"primary_keys_source\"],\n",
    "            primary_keys_bronze=t[\"primary_keys_bronze\"],\n",
    "            columns_source=t[\"columns_source\"],\n",
    "            columns_bronze=t[\"columns_bronze\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du test de {t['source']} : {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0630de7-0d5e-4048-b85c-e69ba782a94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Affiche les logs pour les processus bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efc4320d-5add-4a07-a385-90a4e6ce1cb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT * FROM {catalog}.logs.bronze_processing_log\n",
    "ORDER BY `timestamp`\n",
    "\"\"\"\n",
    "\n",
    "df = spark.sql(query)\n",
    "df.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6060894154855126,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2.5 Bronze layer - Test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
