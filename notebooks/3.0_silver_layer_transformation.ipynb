{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdf91f7-aad0-4ca2-9afe-ce67acf722a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Détection automatique du catalog Unity et des schémas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef95152-6cb9-41dd-8ae3-5426c5f8b613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalogs = [row.catalog for row in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "unity_catalogs = [c for c in catalogs if c != \"hive_metastore\"]\n",
    "\n",
    "if len(unity_catalogs) == 1:\n",
    "    default_catalog = unity_catalogs[0]\n",
    "else:\n",
    "\n",
    "    default_catalog = next((c for c in unity_catalogs if c.startswith(\"dbw_\")), \"hive_metastore\")\n",
    "\n",
    "dbutils.widgets.text(\"my_catalog\", default_catalog, \"Catalog détecté\")\n",
    "catalog = dbutils.widgets.get(\"my_catalog\")\n",
    "    \n",
    "dbutils.widgets.text(\"my_schema\", \"silver\", \"Schéma Silver\")\n",
    "\n",
    "silver_schema = dbutils.widgets.get(\"my_schema\")\n",
    "bronze_schema = \"bronze\"\n",
    "logs_schema = \"logs\"\n",
    "log_table = \"silver_processing_log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0968376-61a0-4985-8f01-8a874884cb22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Récupération de toutes les tables bronze existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c09c598-e191-4b08-a798-b361fa7249ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_tables = [\n",
    "    row.tableName for row in spark.sql(f\"SHOW TABLES IN {catalog}.{bronze_schema}\").collect()\n",
    "    if row.tableName.startswith(\"bronze_\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acf29761-a12e-49ba-9e32-65cf612e9760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction de log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7247a833-3fb2-4077-bc8e-8acadd95b1ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
    "\n",
    "def log_silver_processing_result(table_name, status, rows_inserted=None, message=None):\n",
    "    schema = StructType([\n",
    "        StructField(\"table_name\", StringType(), False),\n",
    "        StructField(\"timestamp\", TimestampType(), False),\n",
    "        StructField(\"status\", StringType(), False),\n",
    "        StructField(\"rows_inserted\", IntegerType(), True),\n",
    "        StructField(\"message\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    data = [{\n",
    "        \"table_name\": table_name,\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"status\": status,\n",
    "        \"rows_inserted\": int(rows_inserted) if rows_inserted is not None else None,\n",
    "        \"message\": message[:5000] if message else None\n",
    "    }]\n",
    "\n",
    "    df_log = spark.createDataFrame(data, schema=schema)\n",
    "    df_log.write.mode(\"append\").format(\"delta\").saveAsTable(f\"{catalog}.{logs_schema}.{log_table}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8796202-15eb-4373-a0d6-154fe93ac089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fonction SCD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacea3d2-31fa-405d-864d-0ce92ac11923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sha2, concat_ws, current_timestamp, lit, col, row_number, coalesce\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def process_scd2(table_bronze):\n",
    "    try:\n",
    "        # Construction des noms de tables Silver et Bronze\n",
    "        table_suffix = table_bronze.replace(\"bronze_\", \"\")\n",
    "        table_silver = f\"silver_{table_suffix}\"\n",
    "\n",
    "        full_bronze = f\"{catalog}.{bronze_schema}.{table_bronze}\"\n",
    "        full_silver = f\"{catalog}.{silver_schema}.{table_silver}\"\n",
    "\n",
    "        # Chargement des données Bronze\n",
    "        bronze_df = spark.table(full_bronze)\n",
    "\n",
    "        # Définition des colonnes à exclure du hash\n",
    "        colonnes_techniques = [\"ingestion_timestamp\", \"valid_from\", \"valid_to\", \"is_current\", \"hash\"]\n",
    "        columns_to_hash = [c for c in bronze_df.columns if c not in colonnes_techniques]\n",
    "\n",
    "        # Détection des colonnes clé primaire (exclut rowguid explicitement)\n",
    "        primary_keys = [c for c in columns_to_hash if c.lower().endswith(\"id\") and c.lower() != \"rowguid\"]\n",
    "        if not primary_keys:\n",
    "            message = f\"Aucune clé primaire détectée pour {table_bronze}. Table ignorée.\"\n",
    "            log_silver_processing_result(table_silver, \"KO\", message=message)\n",
    "            return\n",
    "\n",
    "        # Calcul du hash uniquement à partir des colonnes métier\n",
    "        bronze_hashed = bronze_df.withColumn(\"hash\", sha2(concat_ws(\"||\", *columns_to_hash), 256))\n",
    "\n",
    "        # Initialisation des tables Silver si elles n'existent pas\n",
    "        if not spark._jsparkSession.catalog().tableExists(full_silver):\n",
    "            silver_initial = bronze_hashed \\\n",
    "                .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "                .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "                .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "            silver_initial.write.format(\"delta\").saveAsTable(full_silver)\n",
    "            log_silver_processing_result(\n",
    "                table_silver, \"OK\",\n",
    "                rows_inserted=silver_initial.count(),\n",
    "                message=\"Table Silver initialisée\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Chargement de la Silver existante active\n",
    "        silver_df = spark.table(full_silver).filter(\"is_current = true\")\n",
    "\n",
    "        # Création des colonnes temporaires pour stabiliser les PK sans toucher aux données métier\n",
    "        for pk in primary_keys:\n",
    "            bronze_hashed = bronze_hashed.withColumn(f\"_pk_{pk}\", coalesce(col(pk).cast(\"string\"), lit(\"__NULL__\")))\n",
    "            silver_df = silver_df.withColumn(f\"_pk_{pk}\", coalesce(col(pk).cast(\"string\"), lit(\"__NULL__\")))\n",
    "\n",
    "        # Détection des lignes modifiées ou nouvelles\n",
    "        join_condition = [col(f\"src._pk_{pk}\") == col(f\"tgt._pk_{pk}\") for pk in primary_keys]\n",
    "        joined_df = bronze_hashed.alias(\"src\").join(\n",
    "            silver_df.alias(\"tgt\"),\n",
    "            on=join_condition,\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        changes_df = joined_df.filter(\"tgt.hash IS NULL OR src.hash != tgt.hash\") \\\n",
    "                              .select(\"src.*\")\n",
    "\n",
    "        # Si aucun changement détecté, log et sortie\n",
    "        if changes_df.isEmpty():\n",
    "            log_silver_processing_result(\n",
    "                table_silver, \"OK\",\n",
    "                rows_inserted=0,\n",
    "                message=\"Aucun changement détecté\"\n",
    "            )\n",
    "            return\n",
    "\n",
    "        # Sécurisation : ne garder qu'une seule version par combinaison de clés\n",
    "        window_spec = Window.partitionBy(*primary_keys).orderBy(\"hash\")\n",
    "        changes_df = changes_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "                               .filter(\"row_num = 1\") \\\n",
    "                               .drop(\"row_num\")\n",
    "\n",
    "        insert_count = changes_df.count()\n",
    "\n",
    "        silver_delta = DeltaTable.forName(spark, full_silver)\n",
    "\n",
    "        # Construction dynamique du merge_condition\n",
    "        merge_condition = \" AND \".join([f\"tgt.{pk} = src.{pk}\" for pk in primary_keys])\n",
    "\n",
    "        # Mise à jour des lignes existantes pour les fermer\n",
    "        silver_delta.alias(\"tgt\").merge(\n",
    "            source=changes_df.alias(\"src\"),\n",
    "            condition=merge_condition\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"tgt.is_current = true\",\n",
    "            set={\n",
    "                \"valid_to\": current_timestamp(),\n",
    "                \"is_current\": lit(False)\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "        # Nettoyage avant insertion dans Silver (retirer les colonnes techniques _pk_*)\n",
    "        changes_to_insert = changes_df \\\n",
    "            .withColumn(\"valid_from\", current_timestamp()) \\\n",
    "            .withColumn(\"valid_to\", lit(None).cast(\"timestamp\")) \\\n",
    "            .withColumn(\"is_current\", lit(True))\n",
    "\n",
    "        cols_to_keep = [c for c in changes_to_insert.columns if not c.startswith(\"_pk_\")]\n",
    "        changes_to_insert = changes_to_insert.select(*cols_to_keep)\n",
    "\n",
    "        # Insertion des nouvelles versions actives\n",
    "        changes_to_insert.write.format(\"delta\").mode(\"append\").saveAsTable(full_silver)\n",
    "\n",
    "        # Log final\n",
    "        log_silver_processing_result(\n",
    "            table_silver, \"OK\",\n",
    "            rows_inserted=insert_count,\n",
    "            message=\"Traitement SCD2 appliqué\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        table_suffix = table_bronze.replace(\"bronze_\", \"\")\n",
    "        table_silver = f\"silver_{table_suffix}\"\n",
    "        log_silver_processing_result(table_silver, \"KO\", message=str(e))\n",
    "\n",
    "# Exécution de la fonction SCD2 en bouclant sur toutes les tables bronze\n",
    "for table in bronze_tables:\n",
    "    process_scd2(table)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5870402350909478,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3.0 Silver layer - Transformation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
